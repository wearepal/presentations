<!doctype html>
<html>
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

    <title>Fairness in Machine Learning</title>

    <link rel="stylesheet" href="dist/reset.css">
    <link rel="stylesheet" href="dist/reveal.css">
    <link rel="stylesheet" href="custom_themes/sussex_dark.css" id="theme">

    <!-- Theme used for syntax highlighting of code -->
    <link rel="stylesheet" href="plugin/highlight/monokai.css">

</head>
<style>
    .citeme {
        float: right;
        color: #fff;
        font-size: 12pt;
        font-style: italic;
        margin: 0;
    }
</style>
<body>
<div id="hidden" style="display:none;">
    <div id="static-content">
        <footer>
            <p><font color=#fff>Conference on Data Science and Optimization 2019, The Fields Institute</font></p>
        </footer>
    </div>
</div>
<div class="reveal"><div class="slides">

    <section data-background-color="#000">
        <h1><span class="highlight">Transparency in Fairness</span></h1>

        <p>Oliver Thomas - ot44 at sussex.ac.uk</p>
        <p>Predictive Analytics Lab (PAL), University of Sussex, UK</p>

        <img src="images/sussex_logo.png" style="width: 14%; border: none;"/>
    </section>

    <section data-background-color="#000" data-markdown><textarea data-template>
        ## Contents

        - What and why of fairness in machine learning
        - Algorithmic fairness definitions
        - Approaches to enforce algorithmic fairness
        - Transparency in algorithmic fairness
    </textarea></section>

    <section data-markdown><textarea data-template>
    ## Take home messages

    - Fairness is _Domain Specific_.
    - If you don't think about bias, it will come back to haunt you.
    - Accuracy only tells part of the story.
    - Transparency can be a positive addition to your model.
    </textarea></section>

    <section data-background-color="#fff">
        <img src="images/pp_mb.png" width=72% title="Pro-Publica - Machine Bias"/>
    </section>

    <section data-background-color="#fff">
        <img width=72% src="images/amazon.png" title="Amazon CV Screening"/>
    </section>

<!--    <section data-background-color="#fff">-->
<!--        <img width=90% src="images/sveaekonomi.png" title="Svea Ekonomi"/>-->
<!--    </section>-->

<!--    <section data-background-color="#fff">-->
<!--        <img src="images/norman.png" width=58% title="Norman"/>-->
<!--    </section>-->

    <section data-background-color="#fff">
        <img src="images/bots-at-the-gate.png" width=40% title="Bots At The Gate"/>
    </section>

    <section data-markdown data-background-color="#000"><textarea data-template>
        ## Fairness in Machine Learning

        - A fair machine learning system takes biased datasets and outputs non-discriminatory decisions to people with differing <span class="highlight">protected attributes</span> such as race, gender, and age
        - For example, ensuring classifier to be equally accurate on male and female populations

        <img width=90% src="images/fairML.png" title="Fair ML"/>
    </textarea></section>



    <section data-markdown data-background-color="#000"><textarea data-template>
        ## Sampling bias &#10132; tyranny of the majority

        - In the <span class="highlight">imSitu</span> situation recognition <span class="highlight">dataset</span>, the activity cooking is over 33\% more likely to involve females than males in a training set, and a trained algorithm further <span class="highlight">amplifies</span> the disparity to 68\%
        <span class="citeme">Zhao et al.: Men also like shopping, EMNLP 2017</span>

        <img src="images/women_also_snowboard/example3.png" title="Men also like shopping"/>
    </textarea></section>

    <section data-markdown data-background-color="#000"><textarea data-template>
        ## Sampling bias &#10132; tyranny of the majority

        - <span class="highlight">The reason is</span>: the algorithm predicts the gender from the activity and not from looking at the person
        <span class="citeme">Anne Hendricks et al.: Women also snowboard, ECCV 2018</span>

        <img src="images/women_also_snowboard/example2.png" title="Women also snowboard"/>
    </textarea></section>

<!--    <section data-markdown data-background-color="#000"><textarea data-template>-->
<!--        ## Sampling bias &#10132; tyranny of the majority-->
<!--        -->
<!--        - In the <span class="highlight">UCI Adult Income dataset</span>, 30\% of the male individuals earn more than 50K per year (high income), however of the female individuals only 11\% have a high income-->
<!--        - If an algorithm is trained on this data, the skewness ratio is <span class="highlight">amplified</span> from 3:1 to 5:1-->
<!--        - Simply removing sensitive attribute <emph>gender</emph> from the training data is not sufficient-->
<!--        -->
<!--        <span class="citeme">Pedreshi, Ruggieri, Turini: Discrimination-aware data mining, KDD, 2008.</span>-->
<!--          <div style="height:20px;font-size:1px;">&nbsp;</div>-->
<!--        <span class="citeme">Calders \& Verwer: Three na&iumlve Bayes approaches for discrimination-free classification, Data Mining and Knowledge Discovery 2010.</span>-->
<!--        <span class="citeme">Kamishima et al.: Fairness-Aware classifier with prejudice remover regularizer, ECML 2012 </span>-->
<!--    </textarea></section>-->


    <section data-markdown data-background-color="#000"><textarea data-template>
        ## Enforcing fairness

        - No matter in what way the data is biased: we want to enforce fairness
          - Idea: just tell the algorithm that it should treat all groups in the same way
        - Question: <span class="highlight">how do we define fairness?</span>
          - Really hard question
          - IN SHORT, it is an application-specific
    </textarea></section>

    <section>
        <p>&nbsp;</p>
        <p>&nbsp;</p>
        <h1>Algorithmic fairness definitions</h1>
    </section>

    <section data-markdown data-background-color="#000"><textarea data-template>
        ## Fairness definitions

        - Discrimination in the law:
         - <span class="highlight">Direct discrimination</span> with respect to intent
         - <span class="highlight">Indirect discrimination</span> with respect to consequences
        <span class="citeme">Article 21, EU Charter of Fundamental Rights</span>
        - From the legal context to algorithmic fairness, e.g.:
         - Removing direct discrimination by <span class="highlight">not using group information at prediction</span>
         - Removing indirect discrimination by enforcing <span class="highlight">equality on the outcomes</span> between groups
    </textarea></section>

    <section data-markdown data-background-color="#000"><textarea data-template>
        ## Several notions of statistical fairness:
        - Statistical parity
        - Equalised odds
        - Equality of opportunity
        - Predictive parity

        _Setup_: Binary Classification

        learn some function $~ f: X \rightarrow \hat{Y}$

        s.t. $~ \hat{Y} \approx Y$

        where $~ S, Y, \hat{Y} \in \\{0,1\\}$
    </textarea></section>

    <section data-markdown data-background-color="#000"><textarea data-template>
        ## Statistical fairness notions

        - Statistical parity (
           $
          \hat{Y} \perp S
          $):
            $
            \text{Pr}(\hat{Y}=1 | S=0) = \text{Pr}(\hat{Y}=1 | S=1)
            $
        - Equalised odds ($
          \hat{Y} \perp S | Y
          $):
            $
            \text{Pr}(\hat{Y}=y | S=0, Y=y) = \text{Pr}(\hat{Y}=y | S=1, Y=y)
            $
        - Predictive parity ($
          Y \perp S | \hat{Y}
          $):
            $
            \text{Pr}(Y=y | S=0, \hat{Y}=y) = \text{Pr}(Y=y | S=1, \hat{Y}=y)
            $
        - <span class="highlight">Let's have all the fairness metrics</span>! If we are fair with regards to all notions of fair, then we're fair... right?
    </textarea></section>

    <section data-markdown data-background-color="#000"><textarea data-template>
        ## Mutual exclusivity by Bayes' rule

        <small>
        $$
        \underbrace{\text{Pr}(Y=1|\hat{Y}=1)}_{\text{Positive Predicted Value (PPV)}} = \frac{\text{Pr}(\hat{Y}=1|Y=1)\overbrace{\text{Pr}(Y=1)}^{\text{Base Rate (BR)}}}{\underbrace{\text{Pr}(\hat{Y}=1|Y=1)}_{\text{True Positive Rate (TPR)}}\text{Pr}(Y=1) + \underbrace{\text{Pr}(\hat{Y}=1|Y=-1)}_{\text{False Positive Rate (FPR)}}(1-\text{Pr}(Y=1))}
        $$
        </small>

        <section>
            Suppose we have FPR<sub>S=1</sub> = FPR<sub>S=0</sub> and TPR<sub>S=1</sub> = TPR<sub>S=0</sub> (<span class="highlight">equalised odds</span>), can we have PPV<sub>S=1</sub> = PPV<sub>S=0</sub> (<span class="highlight">predictive parity</span>)?
        </section>
        <section>
            YES! But only if we have a <span class="highlight">perfect dataset</span> (i.e. BR<sub>S=1</sub> = BR<sub>S=0</sub>) or a <span class="highlight">perfect predictor</span> (i.e. FPR=0 and TPR=1 for S=1 and S=0)
            <span class="citeme">Kehrenberg, Chen, NQ: Tuning fairness by marginalizing latent target labels, Oct 2018</span>
            <div style="height:20px;font-size:1px;">&nbsp;</div>
            <span class="citeme">Roth, Impossibility results in fairness as Bayesian inference, Feb 2019</span>
        </section>

    </textarea></section>

    <section data-markdown data-background-color="#000"><textarea data-template>
        ## Mutual exclusivity by Bayes' rule
        <small>
        $$
        \overbrace{\text{Pr}(\hat{Y}=1)}^{\text{Acceptance/Positive Rate (AR or PR)}} = \underbrace{\text{Pr}(\hat{Y}=1|Y=1)}_{\text{TPR}}\underbrace{\text{Pr}(Y=1)}_{\text{Base Rate (BR)}} + \underbrace{\text{Pr}(\hat{Y}=1|Y=-1)}_{\text{FPR}}(1-\text{Pr}(Y=1))
        $$
        </small>

        <section>
        Suppose we have FPR<sub>S=1</sub> = FPR<sub>S=0</sub> and TPR<sub>S=1</sub> = TPR<sub>S=0</sub> (<span class="highlight">equalised odds</span>), can we have AR<sub>S=1</sub> = AR<sub>S=0</sub> (<span class="highlight">statistical parity</span>)?
        </section>

        <section>
        YES! But only if we have a <span class="highlight">perfect dataset</span> (i.e. BR<sub>S=1</sub> = BR<sub>S=0</sub>)

        <span class="citeme">Kehrenberg, Chen, NQ: Tuning fairness by marginalizing latent target labels, Oct 2018</span>
        <div style="height:20px;font-size:1px;">&nbsp;</div>
        <span class="citeme">Roth, Impossibility results in fairness as Bayesian inference, Feb 2019</span>
        </section>

    </textarea></section>

    <section data-markdown data-background-color="#000"><textarea data-template>
        ## So... We just need a perfect dataset?

        Perfect is difficult.

        Requires $~ Y \perp S ~$ which is rarely observed.
    </textarea></section>

    <section data-markdown data-background-color="#000"><textarea data-template>
        ## Which fairness notion to use?
        - There's no right answer, all the above are "fair".
        - It's important to consult <span class="highlight">domain experts</span> to find which is the best fit for each problem.
        - There is no one-size fits all fairness metric.
        - There is also a notion of <span class="highlight">individual fairness</span> (similar individuals to be treated similarly), and
        metrics <span class="highlight">in-between</span> statistical and individual fairness.
    </textarea></section>

    <section>
        <p>&nbsp;</p>
        <p>&nbsp;</p>
        <h1>Algorithmic fairness methods</h1>
    </section>

    <section data-markdown data-background-color="#000"><textarea data-template>
        ## How to enforce fairness?

        <span class="highlight">Three</span> different ways to enforce fairness:

        <img src="images/fairmethods.png" width=75% title="Fair Methods"/>

    </textarea></section>

    <section data-markdown data-background-color="#000"><textarea data-template>
        ## Pre-processing

        - Simplest pre-processing approach is to <span class="highlight">reweight</span> training data points, those with higher weight are used more often and vice versa with lower weight.
        - For example, the weight for a data point with $S=0$ and $Y=1$ is:
        $$
        W(S=0,Y=1) = \frac{\text{Pr}(Y=1)\text{Pr}(S=0)}{\text{Pr}(Y=1,S=0)} = \frac{\\#(S=0)\\#(Y=1)}{\\#(S=0 \land Y=1)}
        $$
        - To make the reweighted dataset to be a <span class="highlight">perfect dataset</span> ($Y \perp S$)

        <span class="citeme">Kamiran and Calders, Data preprocessing techniques for classification without discrimination, KAIS 2012</span>
    </textarea></section>

    <section data-markdown data-background-color="#000"><textarea data-template>
        ## Pre-processing

        - Another popular approach is to produce a "fair" representation
        - Consider that we have 2 roles, a <span class="highlight">data vendor</span>, who is charge of collecting the data and preparing it
        - Our other role is a <span class="highlight">data user</span>, someone who will be making predictions based on our data
        - The data vendor is concerned that the data user may be using their data to make unfair decisions. So the <span class="highlight">data vendor decides to learn a new, fair representation</span>
    </textarea></section>

    <section data-markdown data-background-color="#000"><textarea data-template>
        ## Pre-processing

        - Many <span class="highlight">adversarial-based</span> fair representation learning approaches, e.g. using a "Gradient-Reversal Layer"

        <img src="images/models/adversarialfair.png" width=38% title="Adversarially Fair"/>

        <span class="citeme">Ganin et al.,Domain-adversarial training of neural networks, JMLR 2016</span>
        <div style="height:20px;font-size:1px;">&nbsp;</div>
        <span class="citeme">Edwards and Storkey, Censoring representations with an adversary, ICLR 2016</span>
        <div style="height:20px;font-size:1px;">&nbsp;</div>
        <span class="citeme">Beutel et al., Data decisions and theoretical implications when adversarially learning fair representations, Jul 2017</span>
        <span class="citeme">Madras et al., Learning adversarially fair and transferable representations, ICML 2018</span>

    </textarea></section>

    <section data-markdown data-background-color="#000"><textarea data-template>
        ## Problems?

        Can we visualise/understand the latent representation $z$?
    </textarea></section>


    <section>
        <p>&nbsp;</p>
        <p>&nbsp;</p>
        <h1>Transparency in fairness</h1>
    </section>



    <section>
        <section data-markdown data-background-color="#000"><textarea data-template>
        ## Fair in the data domain
        - A fair representation $T_\omega(X)$ which has the sensitive attribute removed, but remains in the space of $X$
        - Main disentangling assumption:
        $\phi(X) = \phi(T_{\omega}(X)) + \underbrace{\phi (\lnot T_{\omega}(X))}_{\text{Spurious}}
        $
         - <span class="highlight">Spurious</span>: dependent on the sensitive attribute.
         - <span class="highlight">Non-Spurious</span>: independent of the sensitive attribute.

          <span class="citeme">NQ, Sharmanska, Thomas, Discovering fair representations in the data domain, CVPR 2019</span>

        </textarea></section>

        <section data-markdown data-background-color="#000"><textarea data-template>
            ## Fair in the data domain

            <img src="images/models/Architecture.png" width=70% title="Architecture"/>

            - <span class="highlight">No GANs?</span> To perform translation from the input to the non-spurious
            domain via GANs, we need to have real data (e.g. images, tabular data) of both domains

        </textarea></section>

        <section data-markdown data-background-color="#000"><textarea data-template>
        ## Fair in the data domain

        The optimisation problem:

        $\min_{T_\omega} \sum_{n=1}^N\mathcal{L}(T_{\omega}(\mathbf{x}^n),y^n)$

        $+ \lambda_1 \sum_{n=1}^{N}\|\mathbf{x}^n-T_{\omega}(\mathbf{x}^n)\|_2^2$

        $+ \lambda_2 \\|(- \mathrm{HSIC}(\phi (\lnot T_{\omega}(\mathbf{x})),S|Y=+1) + \mathrm{HSIC}(\phi ( T_{\omega}(\mathrm{x})),S|Y=+1)\\|)$
        </textarea></section>

        <section data-markdown data-background-color="#000"><textarea data-template>
            ## Fair in the data domain

             Experiments on the <span class="highlight">Adult Income dataset</span>
            - This dataset is frequently used in fairness research.
            - It has $45,222$ data points, each with $14$ features such as relationship status, gender, occupation.
            - We use gender as a binary sensitive attribute.
            - The binary task is to predict whether a person is a high-earner (>\$50K p.a.) or not.
        </textarea></section>

        <section data-background-color="#000">
            <h2>Results on the adult income dataset</h2>
            <table>
                <tbody><tr>
                    <td width="50%">
                        <p>- Analysis on the relationship feature</p>
                        <p>- Feature values of the <span class="highlight">minority group are transformed to match the majority group</span></p>
                        <p>- Here, the wife value is translated to husband</p></td>
                    <td width="50%">  <img src="images/output.png" width=70% title="outputs"/></td>
                </tr>
                </tbody>
            </table>
        </section>

        <section data-background-color="#000">
            <h2>Results on the adult income dataset</h2>
            <p>Interpretable can be fair!</p>

            <small>
                <table>
                    <thead><tr>
                        <td></td>
                        <td colspan="2" >original $X$</td>
                        <td colspan="2"><span class="highlight">fair & interpretable $X$</span></td>
                        <td colspan="2">latent embedding $Z$</td>
                    </tr>
                    </thead>
                    <tbody><tr>
                        <td></td>
                        <td width="10%">Accuracy $\uparrow$</td>
                        <td width="10%">Eq. Opp $\downarrow$</td>
                        <td width="10%">Accuracy $\uparrow$</td>
                        <td width="10%">Eq. Opp $\downarrow$</td>
                        <td width="10%">Accuracy $\uparrow$</td>
                        <td width="10%">Eq. Opp $\downarrow$</td>
                    </tr>
                    <tr>
                        <td width="10%">LR</td>
                        <td width="10%">$85.1\pm0.2$ </td>
                        <td width="10%">$\mathbf{9.2\pm2.3}$ </td>
                        <td width="10%">$84.2\pm0.3$ </td>
                        <td width="10%">$\mathbf{5.6\pm2.5}$  </td>
                        <td width="10%">$81.8\pm2.1$</td>
                        <td width="10%">$\mathbf{5.9\pm4.6}$ </td>
                    </tr>
                    <tr>
                        <td width="10%">SVM</td>
                        <td width="10%">$85.1\pm0.2$ </td>
                        <td width="10%">$\mathbf{8.2\pm2.3}$ </td>
                        <td width="10%">$84.2\pm0.3$ </td>
                        <td width="10%">$\mathbf{4.9\pm2.8}$ </td>
                        <td width="10%">$81.9\pm2.0$ </td>
                        <td width="10%">$\mathbf{6.7\pm4.7}$</td>
                    </tr>
                    <tr>
                        <td width="20%">Fair Reduction LR</td>
                        <td width="10%">$85.1\pm0.2$ </td>
                        <td width="10%">$\mathbf{14.9\pm1.3}$ </td>
                        <td width="10%">$84.1\pm0.3$ </td>
                        <td width="10%">$\mathbf{6.5\pm3.2}$ </td>
                        <td width="10%">$81.8\pm2.1$ </td>
                        <td width="10%">$\mathbf{5.6\pm4.8}$ </td>
                    </tr>
                    <tr>
                        <td width="10%">Fair Reduction SVM</td>
                        <td width="10%">$85.1\pm0.2$ </td>
                        <td width="10%">$\mathbf{8.2\pm2.3}$ </td>
                        <td width="10%">$84.2\pm0.3$ </td>
                        <td width="10%">$\mathbf{4.9\pm2.8}$ </td>
                        <td width="10%">$81.9\pm2.0$ </td>
                        <td width="10%">$\mathbf{6.7\pm4.7}$ </td>
                    </tr>
                    <tr>
                        <td width="10%">Kamiran & Calders LR</td>
                        <td width="10%">$84.4\pm0.2$ </td>
                        <td width="10%">$\mathbf{14.9\pm1.3}$  </td>
                        <td width="10%">$84.1\pm0.3$ </td>
                        <td width="10%">$\mathbf{1.7\pm1.3}$ </td>
                        <td width="10%">$81.8\pm2.1$ </td>
                        <td width="10%">$\mathbf{4.9\pm3.3}$ </td>
                    </tr>
                    <tr>
                        <td width="10%">Kamiran & Calders SVM</td>
                        <td width="10%">$85.1\pm0.2$ </td>
                        <td width="10%">$\mathbf{8.2\pm2.3}$ </td>
                        <td width="10%">$84.2\pm0.3$ </td>
                        <td width="10%">$\mathbf{4.9\pm2.8}$ </td>
                        <td width="10%">$81.9\pm2.0$ </td>
                        <td width="10%">$\mathbf{6.7\pm4.7}$ </td>
                    </tr>
                    <tr>
                        <td width="10%">Zafar et al.</td>
                        <td width="10%">$85.0\pm0.3$ </td>
                        <td width="10%">$\mathbf{1.8\pm0.9}$ </td>
                        <td width="10%">--- </td>
                        <td width="10%">--- </td>
                        <td width="10%">--- </td>
                        <td width="10%">--- </td>
                    </tr></tbody>
                </table>
            </small>
        </section>
        <section data-markdown data-background-color="#000"><textarea data-template>
## Fair in the data domain

Experiments on <span class="highlight"> CelebA dataset</span>
- $202,599$ celebrity images with $40$ attributes.
- We use gender as a binary sensitive attribute.
- We use the subjective attribute memorability as the classification task.
</textarea></section>
        <section data-markdown data-background-color="#000"><textarea data-template>
## Results on the CelebA dataset

- <span class="highlight">Left</span>: Examples of the spurious residual and non-spurious translation
- <span class="highlight">Right</span>: In the semantic attribute domain ... same conclusion (<span class="highlight">changes in the eyes and lips regions</span>)
<center>
  <table style="border-collapse: collapse; border: none;">
        <tbody ><tr style="border: none;"><td width="40%" colspan="4" style="border: none;"></td><td width="1%" style="border: none;"></td><td width="40%" style="border: none;"></td></tr>
        <tr style="border: none;">
            <td width="10%" style="border: none;">Non-spurious</td>
            <td width="20%" style="border: none;"><img src="images/celeba_res/006126.jpg" width=70% title="Im1"/></td>
            <td width="20%" style="border: none;"><img src="images/celeba_res/015365.jpg" width=70% title="Im1"/></td>
            <td width="20%" style="border: none;"><img src="images/celeba_res/015505.jpg" width=70% title="Im1"/></td>
            <td width="1%" rowspan="2" style="border: none;"><hr width="2" size="250"></td>
            <td width="30%" rowspan="2" style="border: none;"><img src="images/features.png" width=100% title="features"/></td>
        </tr>
        <tr style="border: none;">
            <td width="10%" style="border: none;">Spurious</td>
            <td width="20%" style="border: none;"><img src="images/celeba_res/006126res.jpg" width=60% title="RIm1"/></td>
            <td width="20%" style="border: none;"><img src="images/celeba_res/015365res.jpg" width=60% title="RIm1"/></td>
            <td width="20%" style="border: none;"><img src="images/celeba_res/015505res.jpg" width=60% title="RIm1"/></td>
        </tr></tbody>
    </table></center>
</textarea></section>
    </section>

        <section>
            <h2>Problems?</h2>
            <h4>Spurious and non-spurious visualisations are not exciting!</h4>
            <h4>Residual unfairness (transferability)</h4>
            <img src="images/cmnist%20train.png" width="30%">
            <img src="images/cmnist%20test.png" width="30%">
        </section>

        <section data-markdown data-background-color="#000"><textarea data-template>
            ## Fair, and transferable... Idea 1
            We use deep generative modelling:

            $\mathcal{L} = \mathbb{E}{q_{\theta_{enc}}(z|x)}[\log p_{\theta_{dec}}(x|z, s) - \log p_{\theta_{dec}}(s|z)]- \beta D_{KL}(q_{\theta_{enc}}(z |x) \| p(z))$
            - Representing data points in a latent space
            - Disentangling the latent space (previous work was on the reconstruction space) into two components:
             - <span class="highlight">Spurious</span>: dependent on the sensitive attribute.
             - <span class="highlight">Non-Spurious</span>: independent of the sensitive attribute.
        </textarea></section>

        <section data-markdown data-background-color="#000"><textarea data-template>
            ## Fair, and transferable... Idea 1
            <img src="images/models/CVAE.png" width="60%">
        </textarea></section>

        <section data-markdown data-background-color="#000"><textarea data-template>
            ## Fair, and transferable... Idea 1
            <img src="images/train_original_x_31500.png" width="30%">
            <img src="images/train_reconstruction_y_31500.png" width="30%">
            <img src="images/train_reconstruction_s_31500.png" width="30%">
        </textarea></section>

        <section data-markdown data-background-color="#000"><textarea data-template>
            ## Fair, and transferable... Idea 2
            We use deep generative modelling:

            $\min_\theta \mathbb{E}_x (- \log p_\theta (x|z)) + \lambda I(z_d, s)$

            - Representing data points in a latent space
            - Disentangling the latent space (previous work was on the reconstruction space) into two components:
             - <span class="highlight">Spurious</span>: dependent on the sensitive attribute.
             - <span class="highlight">Non-Spurious</span>: independent of the sensitive attribute.
        </textarea></section>

        <section data-markdown data-background-color="#000"><textarea data-template>
            ## Fair, and transferable... Idea 2
            <img src="images/models/CFlow.png" width="30%">
        </textarea></section>

        <section data-markdown data-background-color="#000"><textarea data-template>
            ## Fair, and transferable... Idea 2
            <img src="images/train_original_x_15000.png" width="30%">
            <img src="images/train_reconstruction_y_15000.png" width="30%">
            <img src="images/train_reconstruction_s_15000.png" width="30%">
        </textarea></section>


    <section data-background-color="#000">
        <section data-markdown data-background-color="#000"><textarea data-template>
## Contrastive examples

- The <span class="highlight">ideal dataset</span> contains an imaginary data point for each person, i.e. the one inside the black box, whereby we <span class="highlight">intervene</span> and set the gender attribute to the opposite that is in real life
<span class="citeme">Thomas, NQ, Imagined Examples for Fairness: Sampling Bias and Proxy Labels, Sep 2019</span>
<span class="citeme">Sharmanska, Hendricks, Darrell, NQ, Contrastive examples for addressing the tyranny of the majority, May 2019</span>


  <img src="images/balanced_Page_1.png" width=50% title="Balanced1"/>
  <img src="images/balanced_Page_2.png" width=30% title="Balanced2"/>



  </textarea></section>
        <section data-markdown><textarea data-template>
## Contrastive on Adult Income dataset

- Change in the <span class="highlight">relationship</span> and <span class="highlight">marital status</span>, i.e. from Husband and Married-civ-spause in real samples to Unmarried and/or Wife in contrastive samples

<center>
  <img src="images/adult_real.png" width=30% title="Real"/>
  <img src="images/adult_contrastive.png" width=30% title="Contrastive"/>
  <img src="images/adult_diff.png" width=30.5% title="Difference"/>
</center>

  </textarea>
        </section>

        <section data-markdown><textarea data-template>
## Contrastive on Adult Income dataset
- Connections to <span class="highlight">counterfactuals</span> based on the Nabi \& Shpitser's (2018) causal graph
 - To remove bias towards females, the direct effect of gender on income, as well as, the <span class="highlight">effect of gender on income through marital status have to be suppressed</span>
- Experimental results
<small>
  <table>
        <tbody><tr>
            <td></td>
            <td width="10%">Accuracy $\uparrow$</td>
            <td width="10%">TPR Diff. $\downarrow$</td>
            <td width="10%">FPR Diff. $\downarrow$</td>
        </tr>
        <tr>
            <td width="10%">LR (Real)</td>
            <td width="10%">$85.16\pm0.14$ </td>
            <td width="10%">$7.98\pm1.52$ </td>
            <td width="10%">$7.23\pm0.41$</td>
        </tr>
        <tr>
            <td width="10%">Kamiran & Calders LR (Real)</td>
            <td width="10%">$84.37\pm0.28$ </td>
            <td width="10%">$14.3\pm1.16$ </td>
            <td width="10%">$1.17\pm0.29$ </td>
        </tr>
        <tr>
            <td width="20%">LR (Real and NN contrastive)</td>
            <td width="10%">$85.01\pm0.25$ </td>
            <td width="10%">$14.80\pm1.90$ </td>
            <td width="10%">$8.20\pm0.51$ </td>
        </tr>
        <tr>
            <td width="20%">LR (Real and GAN contrastive)</td>
            <td width="10%">$82.48\pm0.44$ </td>
            <td width="10%">$4.95\pm3.67$ </td>
            <td width="10%">$3.94\pm1.33$ </td>
        </tr>
			</tbody>
    </table>
    </small>

  </textarea>
        </section>
        <section data-markdown><textarea data-template>
## Interpretability with contrastive examples

- All previous work with adversarial learning try to <span class="highlight">remove</span> sensitive attributes from data
- Instead, we use adversarial learning to <span class="highlight">generate</span> data points with pre-specified sensitive attributes (contrastive examples)
- Contrastive examples can be easily interpreted because they do have the semantic meaning of the input
</textarea></section>
    </section>

<section data-markdown><textarea data-template>
    ## Take home messages

    - Fairness is _Domain Specific_.
    - If you don't think about bias, it will come back to haunt you.
    - Accuracy only tells part of the story.
    - Transparency can be a positive addition to your model.
</textarea></section>

<section data-markdown data-background-color="#000"><textarea data-template>
    ## Shameless Plug

    Interested in comparing fairness approaches?

    `pip install EthicML`

    ---

    Several PostDoc and PhD student positions will be available soon (for details contact <span class="highlight">n.quadrianto at sussex.ac.uk</span>)

    Topics
    - Uncertainty in fairness
    - Fairness in a dynamic setting
    - Interpretability in fairness
</textarea></section>


</div></div>

<script type="module" src="settings.js"></script>
</body>
</html>
